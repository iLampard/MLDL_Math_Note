\chapter{附录：信息熵}
\label{app:entory}
\typeout{START_CHAPTER "model" \theabspage}

假设
\footnote{本章参考了信息论与编码(http://www.docin.com/p-957983839-f6.html)和信息论基础(https://wenku.baidu.com/view/5319fed3b9f3f90f76c61b1a.html)}
$X$是一个取有限值的离散随机变量(本文只考虑离散情况)， 概率分布为$P(X=x_{i})=p_{i},i=1...n$。 

那么$I(x_{i})=-\log p(x_{i})$称为事件$x_{i}$的\newterm{自信息量}， 随机变量$X$ 的\newterm{熵}定义为$X$ 的自信息量的数学期望，即
\[
    H(X)=\E(I(x_{i})) = - \xsum{i=1}{n} p_{i} \log p_{i}
    \nonumber
\]

熵反映的是随机变量不确定程度的大小：熵的值越大，不确定程度越高。

\section{相关概念}

\subsection{条件熵}

\newterm{条件熵}是指在联合概率空间上熵的条件自信息的数学期望。在已知$X$时，$Y$的条件熵为
\[
    H(Y| X)=\E_{\rx,\ry} I(y_{j}|x_{i}) = - \sum_x \sum_y P(x, y) \log P(y|x)
    \label{eqn:cond_entropy}
\]



\begin{lemma}
    与\eqref{eqn:cond_entropy}等价的定义为给定$X$条件下$Y$的条件分布概率的熵的数学期望
    \[
        H(Y| X)=\E_{\rx} H(Y|X=x)=\sum_x P(x)H(Y|X=x)
        \nonumber
    \]
\end{lemma}

\begin{proof}
\[
    H(Y| X)
    =&  - \sum_x \sum_y P(x, y) \log P(y|x)\\
    =& -\sum_x \sum_y P(x)P(y | x) \log P(y | x) \\
    =& -\sum_x P(x)\sum_y P(y | x)\log P(y | x) \quad \prt{\textrm{$P(x)$与$y$无关}}\\
    =& \sum_x P(x)[-\sum_y P(y | x) \log P(y | x)]  \\
    =& \sum_x P(x) H(Y| X=x)  
    \nonumber
\]    
\end{proof}


$H(Y| X)$的含义是已知在$X$发生的前提下，$Y$发生\newterm{新带来的熵}。

\subsection{相对熵}

\newterm{相对熵}，也称\newterm{KL散度}，交叉熵等，定义为两个概率分布之比的数学期望。

设$Q(x),P(x)$是随机变量$X$中取值的两个概率分布，则$P$对$Q$的相对熵是
\[
    \KL ( P \Vert Q )= \sum_x P(x) \log \frac{P(x)}{Q(x)} = \E_{\rx} \log\frac{P(x)}{Q(x)}    
    \label{eqn:kl_def}
\]

相对熵可以用来度量两个随机变量的“距离”。

\begin{lemma}
    相对熵恒大于等于零。
\end{lemma}
\begin{proof}
    对于任意分布$P,Q$，根据\eqref{eqn:kl_def}，可知
    \[
        \KL ( P \Vert Q ) 
        =& \sum_x P(x) \log \frac{P(x)}{Q(x)} \\
        =& -\sum_x P(x) \log \frac{Q(x)}{P(x)} \\
        \ge & -\log(\sum_x P(x) \frac{Q(x)}{P(x)}) \quad \prt{\textrm{对$-logx$应用Jensen不等式} }\\
        =& -\log \sum_x Q(x) \\
        =& -\log1 \\
        =& 0 
        \nonumber
    \]
    
\end{proof}

\subsection{互信息}
两个随机变量$X,Y$的\newterm{互信息}，定义为$X,Y$的联合分布和独立分布乘积的相对熵
\[
    I(X,Y) = \KL(P(X,Y) \Vert P(X)P(Y) )
\]

\begin{lemma}
    互信息与条件熵满足如下关系
    \[
        H(X|Y) = H(X) - I(X,Y)
    \]
\end{lemma}
\begin{proof}
    根据\eqref{eqn:kl_def}以及互信息的定义可知
    \[
        I(X,Y) = \sum_{x,y} P(x,y)\log\frac{P(x,y)}{P(x)P(y)}
        \nonumber
    \]

    那么
    \[
        H(X)-I(X,Y) 
        =& -\sum_x P(x)\log P(x) -\sum_{x,y} P(x,y)\log\frac{P(x,y)}{P(x)P(y)} \\
        =& -\sum_x \prt{\sum_y P(x,y)}\log P(x)-\sum_{x,y} P(x,y)\log\frac{P(x,y)}{P(x)P(y)} \\
        =& -\sum_{x,y}P(x,y)\log P(x) - \sum_{x,y}P(x,y)\log\frac{P(x,y)}{P(x)P(y)} \\
        =& -\sum_{x,y}P(x,y)\prt{\log P(x) + \log\frac{P(x,y)}{P(x)P(y)}} \\
        =& -\sum_{x,y}P(x,y)\log\frac{P(x,y)}{P(y)} \\
        =& -\sum_{x,y}P(x,y)\log  P(x \mid y) \\
        =& H(X|Y)    \quad \prt{\textrm{根据\eqref{eqn:cond_entropy}}}
        \nonumber 
    \]
\end{proof}















